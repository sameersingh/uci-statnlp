{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: POS Tagging\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Part-of-speech_tagging):\n",
    "\n",
    "> Part-of-speech tagging (POS tagging or PoS tagging or POST) is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its contextâ€”i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph.\n",
    "\n",
    "Formally, given a sequence of words $\\mathbf{x} = \\left< x_1, x_2, \\ldots, x_t \\right>$ the goal is to learn a model $P(y_i \\,|\\, \\mathbf{x})$ where $y_i$ is the POS tag associated with the $x_i$.\n",
    "Note that the model is conditioned on all of $\\mathbf{x}$ not just the words that occur earlier in the sentence - this is because we can assume that the entire sentence is known at the time of tagging.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will train our model on the [Engligh Dependencies Treebank](https://github.com/UniversalDependencies/UD_English).\n",
    "You can download this dataset by running the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-dev.conllu\n",
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-test.conllu\n",
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-train.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual data instances come in chunks seperated by blank lines. Each chunk consists of a few starting comments, and then lines of tab-seperated fields. The fields we are interested in are the 1st and 3rd, which contain the tokenized word and POS tag respectively. An example chunk is shown below:\n",
    "\n",
    "```\n",
    "# sent_id = answers-20111107193044AAvUYBv_ans-0023\n",
    "# text = Hope you have a crapload of fun!\n",
    "1\tHope\thope\tVERB\tVBP\tMood=Ind|Tense=Pres|VerbForm=Fin\t0\troot\t0:root\t_\n",
    "2\tyou\tyou\tPRON\tPRP\tCase=Nom|Person=2|PronType=Prs\t3\tnsubj\t3:nsubj\t_\n",
    "3\thave\thave\tVERB\tVBP\tMood=Ind|Tense=Pres|VerbForm=Fin\t1\tccomp\t1:ccomp\t_\n",
    "4\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t5\tdet\t5:det\t_\n",
    "5\tcrapload\tcrapload\tNOUN\tNN\tNumber=Sing\t3\tobj\t3:obj\t_\n",
    "6\tof\tof\tADP\tIN\t_\t7\tcase\t7:case\t_\n",
    "7\tfun\tfun\tNOUN\tNN\tNumber=Sing\t5\tnmod\t5:nmod\tSpaceAfter=No\n",
    "8\t!\t!\tPUNCT\t.\t_\t1\tpunct\t1:punct\t_\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most real world data, we are going to need to do some preprocessing before we can use it. The first thing we are going to need is a `Vocabulary` to map words/POS tags to integer ids. Here is a more full-featured implementation than what we used in the first tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "            iter: An iterable which produces sequences of tokens used to update\n",
    "                the vocabulary.\n",
    "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
    "            sos_token: (Optional) Token denoting the start of a sequence.\n",
    "            eos_token: (Optional) Token denoting the end of a sequence.\n",
    "            unk_token: (Optional) Token denoting an unknown element in a\n",
    "                sequence.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Add special tokens.\n",
    "        id2word = [self.pad_token]\n",
    "        if sos_token is not None:\n",
    "            id2word.append(self.sos_token)\n",
    "        if eos_token is not None:\n",
    "            id2word.append(self.eos_token)\n",
    "        if unk_token is not None:\n",
    "            id2word.append(self.unk_token)\n",
    "\n",
    "        # Update counter with token counts.\n",
    "        counter = Counter()\n",
    "        for x in iter:\n",
    "            counter.update(x)\n",
    "\n",
    "        # Extract lookup tables.\n",
    "        if max_size is not None:\n",
    "            counts = counter.most_common(max_size)\n",
    "        else:\n",
    "            counts = counter.items()\n",
    "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "        words = [x[0] for x in counts]\n",
    "        id2word.extend(words)\n",
    "        word2id = {x: i for i, x in enumerate(id2word)}\n",
    "\n",
    "        self._id2word = id2word\n",
    "        self._word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
    "        Args:\n",
    "            word: Word to lookup.\n",
    "        Returns:\n",
    "            id: The integer id of the word being looked up.\n",
    "        \"\"\"\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        elif self.unk_token is not None:\n",
    "            return self._word2id[self.unk_token]\n",
    "        else:\n",
    "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
    "\n",
    "    def id2word(self, id):\n",
    "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
    "        Args:\n",
    "            id: Integer id of the word being looked up.\n",
    "        Returns:\n",
    "            word: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self._id2word[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parse the .conllu files and extract the data needed for our model. The good news is that the file is only a few megabytes so we can store everything in memory. Rather than creating a generator from scratch like we did in the previous tutorial, we will instead showcase the `torch.utils.data.Dataset` class. There are two main things that a `Dataset` must have:\n",
    "\n",
    "1. A `__len__` method which let's you know how many data points are in the dataset.\n",
    "2. A `__getitem__` method which is used to support integer indexing.\n",
    "\n",
    "Here's an example of how to define these methods for the English Dependencies Treebank data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, fname):\n",
    "        \"\"\"Initializes the CoNLLDataset.\n",
    "        Args:\n",
    "            fname: The .conllu file to load data from.\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.annotations = self.process_conll_file(fname)\n",
    "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
    "                                 unk_token='<unk>')\n",
    "        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
    "        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n",
    "        return input, target\n",
    "\n",
    "    def process_conll_file(self, fname):\n",
    "        # Read the entire file.\n",
    "        with open(fname, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "        # Split into chunks on blank lines.\n",
    "        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n",
    "        # Process each chunk into an annotation.\n",
    "        annotations = []\n",
    "        for chunk in chunks:\n",
    "            annotation = Annotation()\n",
    "            lines = chunk.split('\\n')\n",
    "            # Iterate over all lines in the chunk.\n",
    "            for line in lines:\n",
    "                # If line is empty ignore it.\n",
    "                if len(line)==0:\n",
    "                    continue\n",
    "                # If line is a commend ignore it.\n",
    "                if line[0] == '#':\n",
    "                    continue\n",
    "                # Otherwise split on tabs and retrieve the token and the\n",
    "                # POS tag fields.\n",
    "                fields = line.split('\\t')\n",
    "                annotation.tokens.append(fields[1])\n",
    "                annotation.pos_tags.append(fields[3])\n",
    "            if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n",
    "                annotations.append(annotation)\n",
    "        return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how this is used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CoNLLDataset('en-ud-train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: [262, 16, 5150, 45, 288, 689, 1126, 4148, 9875, 583, 16, 9876, 4, 3, 6733, 35, 3, 6734, 10, 3, 486, 8, 6735, 4, 742, 3, 2180, 1572, 2]\n",
      "\n",
      "Example target: [7, 2, 7, 2, 8, 1, 3, 7, 7, 7, 2, 7, 2, 6, 1, 5, 6, 1, 5, 6, 1, 5, 7, 2, 5, 6, 8, 1, 2]\n",
      "\n",
      "Translated input: Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at the mosque in the town of Qaim , near the Syrian border .\n",
      "\n",
      "Translated target: PROPN PUNCT PROPN PUNCT ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN ADP PROPN PUNCT ADP DET ADJ NOUN PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input, target = dataset[0]\n",
    "print('Example input: %s\\n' % input)\n",
    "print('Example target: %s\\n' % target)\n",
    "print('Translated input: %s\\n' % ' '.join(dataset.token_vocab.id2word(x) for x in input))\n",
    "print('Translated target: %s\\n' % ' '.join(dataset.pos_vocab.id2word(x) for x in target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main upshot of using the `Dataset` class is that it makes accessing training/test observations very simple. Accordingly, this makes batch generation easy since all we need to do is randomly choose numbers and then grab those observations from the dataset - PyTorch includes a `torch.utils.data.DataLoader` object which handles this for you. In fact, if we were not working with sequential data we would be able to proceed straight to the modeling step from here. However, since we are working with sequential data there is one last pesky issue we need to handle - padding.\n",
    "\n",
    "The issue is that when we are given a batch of outputs from `CoNLLDataset`, the sequences in the batch are likely to all be of different length. To deal with this, we define a custom `collate_annotations` function which adds padding to the end of the sequences in the batch so that they are all the same length. In addition, we'll have this function take care of loading the data into tensors and ensuring that the tensor dimensions are in the order expected by PyTorch.\n",
    "\n",
    "Oh and one last annoying thing - to deal with some of the issues caused by using padded data we will be using a function called `torch.nn.utils.rnn.pack_padded_sequences` in our model later on. All you need to know now is that this function expects our sequences in the batch to be sorted in terms of descending length, and that we know the lengths of each sequence. So we will make sure that the `collate_annotations` function performs this sorting for us and returns the sequence lengths in addition to the input and target tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def pad(sequences, max_length, pad_value=0):\n",
    "    \"\"\"Pads a list of sequences.\n",
    "    Args:\n",
    "        sequences: A list of sequences to be padded.\n",
    "        max_length: The length to pad to.\n",
    "        pad_value: The value used for padding.\n",
    "    Returns:\n",
    "        A list of padded sequences.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sequence in sequences:\n",
    "        padded = sequence + [0]*(max_length - len(sequence))\n",
    "        out.append(padded)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_annotations(batch):\n",
    "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
    "    # Get inputs, targets, and lengths.\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    # Sort by length.\n",
    "    sort = sorted(zip(inputs, targets, lengths),\n",
    "                  key=lambda x: x[2],\n",
    "                  reverse=True)\n",
    "    inputs, targets, lengths = zip(*sort)\n",
    "    # Pad.\n",
    "    max_length = max(lengths)\n",
    "    inputs = pad(inputs, max_length)\n",
    "    targets = pad(targets, max_length)\n",
    "    # Transpose.\n",
    "    inputs = list(map(list, zip(*inputs)))\n",
    "    targets = list(map(list, zip(*targets)))\n",
    "    # Convert to PyTorch variables.\n",
    "    inputs = Variable(torch.LongTensor(inputs))\n",
    "    targets = Variable(torch.LongTensor(targets))\n",
    "    lengths = Variable(torch.LongTensor(lengths))\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "    return inputs, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's see how this is used in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "\n",
      "Columns 0 to 12 \n",
      "   28  1056   262    28    30   104    68   262   487   613  9879   119  1185\n",
      " 9882     3    16  1682  6742  6746  9891    16  1005   104    45  9899     8\n",
      "   10  5153  5150  4152    11    11    46  5150  4154  1681   543     4    69\n",
      "  178    19    45     8    10     3   183    45    51     8  1814  6742    60\n",
      "   11   335   288  9894  9901   743   135  5154  9897     7  1980  3055   157\n",
      " 4149   161   689  3055   178  1006     8  1126     7  9877    12     4   440\n",
      "    5     5  1126  1980     6    10     3     7  9898  9878    31   148    44\n",
      "    3   400  4148    12    50     3  2704   790  3056    32    51   216   143\n",
      "  139  1441  9875  2705    52   206  1816     8     6    22  9880  1683   690\n",
      " 1004    10   583    51  6743     8    72     3    66  3508    61    60     8\n",
      "    4  6739    16  4153  2181  2705    60  9892   722   137   170  2420     3\n",
      "   57     6  9876  1574   882    10  6741  5155   228  1440  1053   133  5151\n",
      "   25  4150     4  1983     4     3   313  1817    31    14  3509    31     8\n",
      "   48    61     3    10    72  1346    14   129    60   152  1344    84     3\n",
      "   22  1127  6733  5156    23    29     3  9893    20     5    10    22  9881\n",
      "   62  3509    35     4    20  3057    86    82    63   213  1815  9900    49\n",
      "  303     4     3  9895     3     5   647    10   110     2     2     2     0\n",
      " 5152     9  6734    58  6744     3    10  5156   231   614     0     0     0\n",
      " 9883    36    10    51    14  2182   881     2     2     0     0     0     0\n",
      " 9884    38     3    70  6745    16     2     0     0     0     0     0     0\n",
      " 9885   179   486     7  9902  1684     0     0     0     0     0     0     0\n",
      " 6736     3     8   370     6     8     0     0     0     0     0     0     0\n",
      "   35   691  6735  1984  5157     3     0     0     0     0     0     0     0\n",
      "    7    12     4    10   883   314     0     0     0     0     0     0     0\n",
      "  354     3   742     3    69     2     0     0     0     0     0     0     0\n",
      "  584  6740     3   206    10    27     0     0     0     0     0     0     0\n",
      "  501   669  2180     8     2     0     0     0     0     0     0     0     0\n",
      "    8     5  1572  9896     0     0     0     0     0     0     0     0     0\n",
      "    3  9890     2     2     0     0     0     0     0     0     0     0     0\n",
      " 9886     3     0     0     0     0     0     0     0     0     0     0     0\n",
      " 6737   543     0     0     0     0     0     0     0     0     0     0     0\n",
      "   99   401     0     0     0     0     0     0     0     0     0     0     0\n",
      "   10    78     0     0     0     0     0     0     0     0     0     0     0\n",
      "    3  4151     0     0     0     0     0     0     0     0     0     0     0\n",
      " 6738     2     0     0     0     0     0     0     0     0     0     0     0\n",
      "    2     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 15 \n",
      "   28    28   107\n",
      " 3509  1054  1573\n",
      "   60    19    54\n",
      " 1344   157    41\n",
      "   10   440    19\n",
      "    3    44  9887\n",
      " 9888     3     3\n",
      "    6   595   570\n",
      "  583     8    21\n",
      "   16    52    66\n",
      " 1263  1981  3054\n",
      " 9889  1345  1982\n",
      "    8     2     2\n",
      "    3     0     0\n",
      " 1055     0     0\n",
      "    2     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "[torch.cuda.LongTensor of size 36x16 (GPU 0)]\n",
      "\n",
      "\n",
      "Targets: \n",
      "\n",
      "Columns 0 to 12 \n",
      "    6    14     7     6     2     6     4     7     5     2     7     5    13\n",
      "    7     6     2     1     7     1     3     2     7     6     2     7     5\n",
      "    5     1     7     1     9     9     4     7     1     1     8     2     4\n",
      "    7     9     2     5     5     6    10     2     9     5     1     7     9\n",
      "    9    10     8     7     8     8    10     1     3     6     3     7     9\n",
      "    8     3     1     7     7     1     5     3     6     8    14     2     3\n",
      "    5    12     3     3    11     5     6     6     8     1     4    13     5\n",
      "    6     3     7    14     3     6     7     1     1     9     9     1    13\n",
      "    7     1     7     1     6     1     1     5    11     9     3     1     1\n",
      "    7    14     7     9    10     5     4     6     4     3     5     9     5\n",
      "    2     3     2     3     8     1     9     7     1     4    13     3     6\n",
      "   10    11     7    13     1     5    10     7    14     1     8    14     7\n",
      "    4     3     2     1     2     6     3     7     4     5     1     4     5\n",
      "    9     5     6     5     4     1     5    14     9     1     3     9     6\n",
      "    3     1     1     7     9    14     6     3     5    12     5     9     7\n",
      "   14     1     5     2     5     3     8     4     4     3     7     3     2\n",
      "    3     2     6    14     6     5     1     5     1     2     2     2     0\n",
      "    7     4     1     4     1     6     5     7    10     2     0     0     0\n",
      "    7     9     5     9     5     1     7     2     2     0     0     0     0\n",
      "    7    12     6     3     7     2     2     0     0     0     0     0     0\n",
      "   10     3     1     6     1     1     0     0     0     0     0     0     0\n",
      "    3     6     5     8    11     5     0     0     0     0     0     0     0\n",
      "    5     1     7     1    10     6     0     0     0     0     0     0     0\n",
      "    6    14     2     5     3     1     0     0     0     0     0     0     0\n",
      "    8     6     5     6     4     2     0     0     0     0     0     0     0\n",
      "    1     7     6     1     5     2     0     0     0     0     0     0     0\n",
      "    1     3     8     5     2     0     0     0     0     0     0     0     0\n",
      "    5    12     1     1     0     0     0     0     0     0     0     0     0\n",
      "    6     3     2     2     0     0     0     0     0     0     0     0     0\n",
      "    7     6     0     0     0     0     0     0     0     0     0     0     0\n",
      "    1     8     0     0     0     0     0     0     0     0     0     0     0\n",
      "   10     1     0     0     0     0     0     0     0     0     0     0     0\n",
      "    5    10     0     0     0     0     0     0     0     0     0     0     0\n",
      "    6     8     0     0     0     0     0     0     0     0     0     0     0\n",
      "    1     2     0     0     0     0     0     0     0     0     0     0     0\n",
      "    2     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 15 \n",
      "    6     6     4\n",
      "    1     8     3\n",
      "    9     9    14\n",
      "    3     9     4\n",
      "    5     3     9\n",
      "    6     5     3\n",
      "    7     6     6\n",
      "   11     1     1\n",
      "    7     5     5\n",
      "    2     6     4\n",
      "    7     1     1\n",
      "    1     1     1\n",
      "    5     2     2\n",
      "    6     0     0\n",
      "    1     0     0\n",
      "    2     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "[torch.cuda.LongTensor of size 36x16 (GPU 0)]\n",
      "\n",
      "\n",
      "Lengths: \n",
      " 36\n",
      " 35\n",
      " 29\n",
      " 29\n",
      " 27\n",
      " 26\n",
      " 20\n",
      " 19\n",
      " 19\n",
      " 18\n",
      " 17\n",
      " 17\n",
      " 16\n",
      " 16\n",
      " 13\n",
      " 13\n",
      "[torch.cuda.LongTensor of size 16 (GPU 0)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "for inputs, targets, lengths in DataLoader(dataset, batch_size=16, collate_fn=collate_annotations):\n",
    "    print('Inputs: %s\\n' % inputs.data)\n",
    "    print('Targets: %s\\n' % targets.data)\n",
    "    print('Lengths: %s\\n' % lengths.data)\n",
    "\n",
    "    # Usually we'd keep sampling batches, but here we'll just break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The model implementation is relatively simple, we will use the following architecture:\n",
    "\n",
    "1. Embed the input words into a 200 dimensional vector space.\n",
    "2. Feed the word embeddings into a (bidirectional) GRU.\n",
    "3. Feed the GRU outputs into a fully connected layer.\n",
    "4. Use a softmax activation to get the probabilities of the different labels.\n",
    "\n",
    "The only complication arises during the forward computation. As was noted in the dataset section, the input sequences are padded. This causes an issue since we do not want to waste computational resources feeding these pad tokens into the RNN. In PyTorch, we can deal with this issue by converting the data into a  `torch.nn.utils.rnn.PackedSequence` object and letting PyTorch take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Tagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_vocab_size,\n",
    "                 output_vocab_size,\n",
    "                 embedding_dim=200,\n",
    "                 hidden_size=1024,\n",
    "                 num_layers=2,\n",
    "                 bidirectional=True):\n",
    "        \"\"\"Initializes the tagger.\n",
    "        \n",
    "        Args:\n",
    "            input_vocab_size: Size of the input vocabulary.\n",
    "            output_vocab_size: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            hidden_size: Number of units in each LSTM hidden layer.\n",
    "            num_layers: Number of hidden layers.\n",
    "            bidirectional: Whether or not to use a bidirectional rnn.\n",
    "        \"\"\"\n",
    "        # Always do this!!!\n",
    "        super(Tagger, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_vocab_size, embedding_dim,\n",
    "                                            padding_idx=0)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2*hidden_size, output_vocab_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the language model.\n",
    "        \n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x.\n",
    "            hidden: Hidden state to be fed into the lstm.\n",
    "            \n",
    "        Returns:\n",
    "            net: Probability of the next word in the sequence.\n",
    "            hidden: Hidden state of the lstm.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "        \n",
    "        # If no hidden state is provided, then default to zeros.\n",
    "        if hidden is None:\n",
    "            if self.bidirectional:\n",
    "                num_directions = 2\n",
    "            else:\n",
    "                num_directions = 1\n",
    "            hidden = Variable(torch.zeros(self.num_layers * num_directions, batch_size, self.hidden_size))\n",
    "            if torch.cuda.is_available():\n",
    "                hidden = hidden.cuda()\n",
    "\n",
    "        net = self.word_embeddings(x)\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.data.view(-1).tolist()\n",
    "            net = pack_padded_sequence(net, lengths)\n",
    "        net, hidden = self.rnn(net, hidden)\n",
    "        if lengths is not None:\n",
    "            net, _ = pad_packed_sequence(net)\n",
    "        net = self.fc(net)\n",
    "        net = self.activation(net)\n",
    "\n",
    "        return net, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is pretty much exactly the same as in the previous tutorial. There is one catch - we don't want to evaluate our loss function on pad tokens. This is easily fixed by setting the weight of the pad class to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentimentDataset' object has no attribute 'pos_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-f46e8823a97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Hyperparameters / constants.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentimentDataset' object has no attribute 'pos_vocab'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hyperparameters / constants.\n",
    "input_vocab_size = len(dataset.token_vocab)\n",
    "output_vocab_size = len(dataset.pos_vocab)\n",
    "batch_size = 16\n",
    "epochs = 25\n",
    "\n",
    "# Initialize the model.\n",
    "model = Tagger(input_vocab_size, output_vocab_size)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss function weights.\n",
    "weight = torch.ones(output_vocab_size)\n",
    "weight[0] = 0\n",
    "if torch.cuda.is_available():\n",
    "    weight = weight.cuda()\n",
    "    \n",
    "# Initialize loss function and optimizer.\n",
    "loss_function = torch.nn.NLLLoss(weight)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Main training loop.\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                         collate_fn=collate_annotations)\n",
    "losses = []\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, lengths=lengths)\n",
    "\n",
    "        outputs = outputs.view(-1, output_vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.data[0])\n",
    "        if (i % 100) == 0:\n",
    "            average_loss = np.mean(losses)\n",
    "            losses = []\n",
    "            print('Iteration %i - Loss: %0.6f' % (i, average_loss), end='\\r')\n",
    "        if (i % 1000) == 0:\n",
    "            torch.save(model, 'pos_tagger.pt')\n",
    "        i += 1\n",
    "        \n",
    "torch.save(model, 'pos_tagger.final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Now let's look at some of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('pos_tagger.final.pt')\n",
    "\n",
    "def inference(sentence):\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    for word, tag in zip(sentence, pos_tags):\n",
    "        print('%s - %s' % (word, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - PRON\n",
      "do - AUX\n",
      "not - PART\n",
      "like - VERB\n",
      "green - ADJ\n",
      "eggs - NOUN\n",
      "and - CCONJ\n",
      "ham - VERB\n",
      ". - PUNCT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlogan/projects/uci-statnlp/tutorials/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I do not like green eggs and ham .\".split()\n",
    "inference(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Sentiment Analysis\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis):\n",
    "\n",
    ">Opinion mining (sometimes known as sentiment analysis or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    "\n",
    "Formally, given a sequence of words $\\mathbf{x} = \\left< x_1, x_2, \\ldots, x_t \\right>$ the goal is to learn a model $P(y \\,|\\, \\mathbf{x})$ where $y$ is the sentiment associated to the sentence. This is very similar to the problem above, with the exception that we only want a single output for each sentence not a sentence. Accordingly, we will only highlight the neccessary changes that need to be made.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will be using the Kaggle 'Sentiment Analysis on Movie Reviews' dataset [[link](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)]. You will need to agree to the Kaggle terms of service in order to download this data. Since we've already covered the material, we'll skip over the data processing stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
    "        self.tokens = []\n",
    "        self.sentiment = None\n",
    "\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, fname):\n",
    "        \"\"\"Initializes the SentimentDataset.\n",
    "        Args:\n",
    "            fname: The .tsv file to load data from.\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.annotations = self.process_tsv_file(fname)\n",
    "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
    "                                 unk_token='<unk>')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
    "        target = annotation.sentiment\n",
    "        return input, target\n",
    "\n",
    "    def process_tsv_file(self, fname):\n",
    "        # Read the entire file.\n",
    "        with open(fname, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        annotations = []\n",
    "        observed_ids = set()\n",
    "        for line in lines[1:]:\n",
    "            annotation = Annotation()\n",
    "            _, sentence_id, sentence, sentiment = line.split('\\t')\n",
    "            sentence_id = sentence_id\n",
    "            if sentence_id in observed_ids:\n",
    "                continue\n",
    "            else:\n",
    "                observed_ids.add(sentence_id)\n",
    "            annotation.tokens = sentence.split()\n",
    "            annotation.sentiment = int(sentiment)\n",
    "            if len(annotation.tokens) > 0:\n",
    "                annotations.append(annotation)\n",
    "        return annotations\n",
    "\n",
    "\n",
    "def pad(sequences, max_length, pad_value=0):\n",
    "    \"\"\"Pads a list of sequences.\n",
    "    Args:\n",
    "        sequences: A list of sequences to be padded.\n",
    "        max_length: The length to pad to.\n",
    "        pad_value: The value used for padding.\n",
    "    Returns:\n",
    "        A list of padded sequences.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sequence in sequences:\n",
    "        padded = sequence + [0]*(max_length - len(sequence))\n",
    "        out.append(padded)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_annotations(batch):\n",
    "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
    "    # Get inputs, targets, and lengths.\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    # Sort by length.\n",
    "    sort = sorted(zip(inputs, targets, lengths),\n",
    "                  key=lambda x: x[2],\n",
    "                  reverse=True)\n",
    "    inputs, targets, lengths = zip(*sort)\n",
    "    # Pad.\n",
    "    max_length = max(lengths)\n",
    "    inputs = pad(inputs, max_length)\n",
    "    # Transpose.\n",
    "    inputs = list(map(list, zip(*inputs)))\n",
    "    # Convert to PyTorch variables.\n",
    "    inputs = Variable(torch.LongTensor(inputs))\n",
    "    targets = Variable(torch.LongTensor(targets))\n",
    "    lengths = Variable(torch.LongTensor(lengths))\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "    return inputs, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_vocab_size,\n",
    "                 output_vocab_size,\n",
    "                 embedding_dim=200,\n",
    "                 hidden_size=1024):\n",
    "        \"\"\"Initializes the tagger.\n",
    "        \n",
    "        Args:\n",
    "            input_vocab_size: Size of the input vocabulary.\n",
    "            output_vocab_size: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            hidden_size: Number of units in each LSTM hidden layer.\n",
    "        \"\"\"\n",
    "        # Always do this!!!\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_vocab_size, embedding_dim,\n",
    "                                            padding_idx=0)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the language model.\n",
    "        \n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x.\n",
    "            hidden: Hidden state to be fed into the lstm.\n",
    "            \n",
    "        Returns:\n",
    "            net: Probability of the next word in the sequence.\n",
    "            hidden: Hidden state of the lstm.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "        \n",
    "        # If no hidden state is provided, then default to zeros.\n",
    "        if hidden is None:\n",
    "            hidden = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "            if torch.cuda.is_available():\n",
    "                hidden = hidden.cuda()\n",
    "\n",
    "        net = self.word_embeddings(x)\n",
    "        if lengths is not None:\n",
    "            lengths_list = lengths.data.view(-1).tolist()\n",
    "            net = pack_padded_sequence(net, lengths_list)\n",
    "        net, hidden = self.rnn(net, hidden)\n",
    "        net = self.fc(hidden)\n",
    "        net = self.activation(net)\n",
    "\n",
    "        return net, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss: 1.590926\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlogan/projects/uci-statnlp/tutorials/.venv/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type SentimentClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2600 - Loss: 0.053897\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset.\n",
    "dataset = SentimentDataset('train.tsv')\n",
    "\n",
    "# Hyperparameters / constants.\n",
    "input_vocab_size = len(dataset.token_vocab)\n",
    "output_vocab_size = 5\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the model.\n",
    "model = SentimentClassifier(input_vocab_size, output_vocab_size)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Initialize loss function and optimizer.\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Main training loop.\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                         collate_fn=collate_annotations)\n",
    "losses = []\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, lengths=lengths)\n",
    "\n",
    "        outputs = outputs.view(-1, output_vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.data[0])\n",
    "        if (i % 100) == 0:\n",
    "            average_loss = np.mean(losses)\n",
    "            losses = []\n",
    "            print('Iteration %i - Loss: %0.6f' % (i, average_loss), end='\\r')\n",
    "        if (i % 1000) == 0:\n",
    "            torch.save(model, 'sentiment_classifier.pt')\n",
    "        i += 1\n",
    "        \n",
    "torch.save(model, 'sentiment_classifier.final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('sentiment_classifier.final.pt')\n",
    "\n",
    "def inference(sentence):\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, pred = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        pred = pred.cpu()\n",
    "    pred = pred.data.view(-1).numpy()\n",
    "    print('Sentence: %s' % ' '.join(sentence))\n",
    "    print('Sentiment (0=negative, 4=positive): %i' % pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I do not like green eggs and ham .\n",
      "Sentiment (0=negative, 4=positive): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlogan/projects/uci-statnlp/tutorials/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I do not like green eggs and ham .'.split()\n",
    "inference(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
